# ollama_streamlit
See the demo code [ollama_client_streamlit.py](https://github.com/sekewei/ollama_streamlit/blob/main/ollama_client_streamlit.py) to [Build a Lightweight Streamlit Client for Local Ollama LLM Interaction](https://seke-blog.blogspot.com/2025/06/building-lightweight-streamlit-client.html).
You will need to install [Ollama](https://ollama.com/download) and download the desired large language models to local site first.
![Demo Screenshot](https://github.com/sekewei/ollama_streamlit/blob/020dbe4aca3b09787c27b4010a9c90d72bbea3d8/ollama_streamlit_demo.jpg)
